---
title: '03: Implement Chat History (Context Window)'
layout: default
nav_order: 3
parent: 'Exercise 01: Build a serverless, AI RAG application using data from Azure Cosmos DB'
---


# Task 03: Implement Chat History (Context Window)

## Introduction
A big objective for Boulder is creating a "conversational infotainment system". That means remembering prior user inputs, like recent route selections, so follow-up queries feel natural. Customers rarely ask just one question. A rider might ask about "e-bikes for commuting," then follow up with "Which has the longest battery range?". For the conversation to feel seamless, your Intelligent Assistant must remember what the user already asked. This is where your context window comes in, like building a memory into the bike's onboard computer.
 

## Description
You have the basics for your generative AI application now in place. Next, you'll do further testing and explore how well it responds to natural language interactions.

## Success criteria
 - Your Intelligent Assistant "remembers" previous user prompts, enabling follow-up questions to reference earlier context.
 - You have logic in place to ensure the AI doesn't exceed token limits or slow down.
 - When you ask a second question, the answer reflects details from your first question, proving context is working.

## Learning resources
- [Understanding Token Usage in LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models)  

## Key steps:

### 01: Test contextual follow-up questions

Before adding chat history, a second question won't reference the first, leaving the user confused. Seeing that failure helps you appreciate why CWBC insisted on multi-turn conversations-no one wants to repeat themselves.

Let's explore what happens when you test contextual follow-up questions with our LLM by asking follow-up questions that imply an existing context, like you would have in a conversation with another person.

1. In the same chat session as the previous task, enter: `What are the least expensive?`

    {: .note }
    > The response generated will either have nothing to do with your first question, or the LLM may respond that it needs more context to give you an answer.
    >
    > This demonstrates that LLM's are stateless. They don't maintain any conversation history by themselves and are missing the context necessary for the LLM to respond appropriately to your second question.

1. Close the web browser and end the process in the terminal by selecting **Ctrl+C**.



#### Tokens in Large Language Models

In this task, you'll implement chat history, often called a **Context Window** for a generative AI application. Before you write the code, we need to explain the concept of tokens for an LLM and why these are important to consider when implementing a context window.

LLMs require chat history to generate contextually relevant results, but they have limits on how much text they can process in a request, and output in a response. These limits are not expressed as words, but as **tokens**. Tokens represent words or part of a word. On average 4 characters is one token. Tokens are essentially the compute currency for a large language model.

It's necessary to manage the usage of tokens within your app to stay within the LLM's limits. This can be a bit tricky in certain scenarios. You need to provide enough context for the LLM to generate a correct response, while avoiding negative results of consuming too many tokens such as getting incomplete results or unexpected behavior.

To limit the maximum amount of chat history (and text) we send to our LLM, we'll count the number of user prompts we send to the LLM as context. This app uses a variable **_maxContextWindow** to manage the limit for each request.



### 02: Building a context window using tokens

You'll store recent messages but also mind the token limit. CWBC can't afford slow, bloated requests. Striking a balance ensures your Intelligent Assistant remembers enough about the conversation without bogging down performance.

In this task, you'll implement the **GetSessionContextWindowAsync()** function in the **Services/CosmosDbService.cs** class to build our chat history.

1. In VS Code's **EXPLORER** pane, open **CosmosDbService.cs** from the same **Services** subfolder as the previous files you modified.

    ![arj35xmd.jpg](../../media/arj35xmd.jpg)

1. Select **Ctrl+F** to find the `GetSessionContextWindowAsync`() method with the following signature:

    ```csharp
    public async Task<List<Message>> GetSessionContextWindowAsync(string tenantId, string userId, string sessionId, int maxContextWindow)
    ```

1. Replace the placeholder **string queryText** in this function and add a query, using the following:

    ```csharp
    //string queryText = $"";
    string queryText = $"""
        SELECT Top @maxContextWindow
            *
        FROM c  
        WHERE 
            c.tenantId = @tenantId AND 
            c.userId = @userId AND
            c.sessionId = @sessionId AND 
            c.type = @type
        ORDER BY 
            c.timeStamp DESC
        """;
    ```

    ![ymdesz8a.jpg](../../media/ymdesz8a.jpg)

    {: .note }
    > This selects the most recent number of messages in the chat session depending on the **maxContextWindow** variable.
    >
    > After querying for the most recent messages in Azure Cosmos DB, we put them back in order in reverse. The most recent text is what we want closer to the actual question. Counting the number of messages allows us to control the total number of tokens used while still providing relevant context.

1. Save the **CosmosDbService.cs** file.

1. Go back to the **ChatService.cs** file.

1. Find the `public async Task<Message> GetChatCompletionAsync` line. 

1. Use the following to replace the two lines initializing the **List<Message> messages** variable and passing it to the Semantic Kernel service:

    ```csharp
    //List<Message> messages = new List<Message>() { chatMessage };
    //(chatMessage.Completion, chatMessage.CompletionTokens) = await _semanticKernelService.GetChatCompletionAsync(messages);

    //Get the context window for this conversation up to the maximum conversation depth.
    List<Message> contextWindow = 
        await _cosmosDbService.GetSessionContextWindowAsync(tenantId, userId, sessionId, _maxContextWindow);

    (chatMessage.Completion, chatMessage.CompletionTokens) = await _semanticKernelService.GetChatCompletionAsync(contextWindow);
    ```

    ![zsfgfi6v.jpg](../../media/zsfgfi6v.jpg)

    {: .note }
    > This calls the function to get the context window that you just updated, and passes the context window to the Semantic Kernel Service to get a completion from Azure OpenAI.
    >
    > You're now passing in a list of messages that represents the conversation history to get our contextually relevant completion. You implemented **GetChatCompletionAsync()** to take a list of prompts, rather than a single prompt representing the current user message alone.

1. Save the **ChatService.cs** file.



### 03: Check your work

A test ride with two or three related questions reveals whether the system can keep track. If it references the first question when answering the second, you know chat history is working-just as CWBC envisioned for a smooth user experience.

You're now ready to test your context window implementation.

1. In the VS Code terminal, start the application:

    ```
    dotnet run
    ```

1. **Ctrl+click** the URL on the **Login to the dashboard** line.

1. Select the **http://localhost:8100** endpoint to start the chat application.

1. Select **Create New Chat** on the left, then select the **New Chat** tile that was created.

    ![9n2fevru.jpg](../../media/9n2fevru.jpg)

1. Enter 

    ```
    What are the most expensive bikes?
    ```
    
1. Type this as a follow-up:

    ```
    What are the least expensive?
    ```

    {: .note }
    > You should now see appropriate output similar to the following.

    ![j4yt3rxp.jpg](../../media/j4yt3rxp.jpg)

1. Close the browser.

1. End the process in the terminal by selecting **Ctrl+C**.

<details markdown="block"> 
    <summary>Is your application not working or throwing exceptions? Select here to compare your code against this example.</summary>

 
1. Review the **GetChatCompletionAsync** method of the **ChatService.cs** code file to make sure that your code matches this sample.
 
    ```csharp
    public async Task<Message> GetChatCompletionAsync(string tenantId, string userId, string sessionId, string promptText)
    {
       //Create a message object for the new user prompt and calculate the tokens for the prompt
        Message chatMessage = await CreateChatMessageAsync(tenantId, userId, sessionId, promptText);

        //Get the context window for this conversation up to the maximum conversation depth.
        List<Message> contextWindow = 
            await _cosmosDbService.GetSessionContextWindowAsync(tenantId, userId, sessionId, _maxContextWindow);

        (chatMessage.Completion, chatMessage.CompletionTokens) = await _semanticKernelService.GetChatCompletionAsync(sessionId, contextWindow);

        await UpdateSessionAndMessage(tenantId, userId, sessionId, chatMessage);

        return chatMessage;
    }
    ```
</details>

---

**Congratulations!** You've successfully completed this task.